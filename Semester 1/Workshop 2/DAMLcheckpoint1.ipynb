{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checkpoint 1: Neural networks and deep learning\n",
    "---\n",
    "*Responsible:* Guillermo Hamity (<ghamity@ed.ac.uk>)\n",
    "\n",
    "In this checkpoint exercise, we will use neural networks to predict the **type** of weather *given* the available ground observations. You will be using observation data from **June 2019** across all UK Met Office weather stations.\n",
    "\n",
    "### Notes on the Dataset\n",
    "* You will be using weather observation data from the UK Met Office Datapoint service\n",
    "* Ground observations are made hourly at weather stations across the length of the UK \n",
    "* The data sample covers data from June 2019\n",
    "* Data collections for each day starts at 6.30pm. All observation data is listed in one day blocks\n",
    "* The time value column refers to the number of minutes after midnight \n",
    "* `Null` values for some features are expected (e.g. Wind Gust)\n",
    "* Data import and preparation is already provided \n",
    "\n",
    "\n",
    "This week, I am not providing example notebooks like `lecture2.ipynb` and `data-science-tools.ipynb` for Unit 2, though these may still be useful to you. Instead, I am **providing the imports for all of the modules and classes that you should need.** Think of these as LEGO blocks; you have the ones you need but may look up how to \"assemble\" them.\n",
    "\n",
    "### Notes on assessment\n",
    "* Try and calculate the answers to the exercises provided. If you are unable to complete the question, describe which approach you _would_ have taken to solve the problem\n",
    "* Code must be understandable and reproducible. Before grading the notebook kernel **may** be restarted and re-run, so make sure that your code can run from start to finish without any (unintentional) errors\n",
    "* If you are unsure on how to proceed please **ask one of the TAs** during the workshop\n",
    "- Notebooks should be submitted by **10am on Friday 9 October 2021** \n",
    "- This CP exercise sheet is divided into **6 sections**, corresponding to parts of the lecture, giving a maximum of **10 marks** in total:\n",
    "\n",
    "| <p align='left'> Title                         | <p align='left'> Exercise nos. | <p align='left'> Number of marks |\n",
    "| ------------------------------------- | ----- | --- |\n",
    "| <p align='left'> 1. Conceptual questions               | <p align='left'>  1–5  | <p align='left'> 2.5 |\n",
    "| <p align='left'> 2. Data preprocessing and RandomForest                | <p align='left'>  6–9  | <p align='left'> 2.5 |\n",
    "| <p align='left'> 3. Neural networks in `scikit-learn`  | <p align='left'>  10–11 | <p align='left'> 1.5 | \n",
    "| <p align='left'> 4. Neural networks in `Keras`         | <p align='left'> 12–13 | <p align='left'> 2 |\n",
    "| <p align='left'> 5. Regularisation                     | <p align='left'> 14–15 | <p align='left'> 1.5 |\n",
    "| <p align='left'> 6. Bonus: Hyperparameter optimisation | <p align='left'> 16 | <p align='left'> 1.0 (\\*bonus\\*) |\n",
    "| <p align='left'> **Total** | | <p align='left'> **10 + 1** |\n",
    "\n",
    "- The total number of marks allocated for this CP is 10,\n",
    "    - 1 additional mark can be given (maximimally up to 10 marks in total) for \"bonus\" exercise on hyperparameter optimisation. If you are pressed for time, focus on the first five sections; those are the core ones.\n",
    "    - Half marks may be deducted for code legibility (i.e. very difficult to tell what you are doing), or for badly formated plots (i.e. no legends, axis labels etc.). The TAs will use their discression for this so comment code when applicable and keep relevant information in your plots.\n",
    "\n",
    "_Note:_ You can suppress double-printing of plots from the `plot` module by either _(a)_ adding a semicolon after the function call (_i.e._ `plot.<method>(...);`), or _(b)_ by capturing the return `pyplot.Figure` object as a variable (_i.e._ `fig = plot.<method>(...)`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preamble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard import(s)\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random as rn\n",
    "import sklearn\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "%matplotlib inline\n",
    "\n",
    "# Suppress unnecessary ConvergenceWarnings and DeprecationWarnings\n",
    "import warnings\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "warnings.filterwarnings(action='ignore', category=ConvergenceWarning)\n",
    "warnings.filterwarnings(action='ignore', category=DeprecationWarning)\n",
    "\n",
    "# Set a random seed variable to make workbook reproducible\n",
    "seed=5\n",
    "np.random.seed(seed)\n",
    "rn.seed(seed)\n",
    "os.environ['PYTHONHASHSEED']=str(seed)\n",
    "tf.compat.v1.set_random_seed(seed)\n",
    "\n",
    "# Switch off multi-threading for TensorFlow\n",
    "from tensorflow.python.keras import backend as K\n",
    "config = tf.compat.v1.ConfigProto(intra_op_parallelism_threads=1,\n",
    "                                  inter_op_parallelism_threads=1)\n",
    "sess = tf.compat.v1.Session(graph=tf.compat.v1.get_default_graph(), config=config)\n",
    "K.set_session(sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>StationID</th>\n",
       "      <th>StationName</th>\n",
       "      <th>Elevation</th>\n",
       "      <th>Latitude</th>\n",
       "      <th>Longitude</th>\n",
       "      <th>Date</th>\n",
       "      <th>Time</th>\n",
       "      <th>Gust</th>\n",
       "      <th>Temperature</th>\n",
       "      <th>Visibility</th>\n",
       "      <th>WindDirection</th>\n",
       "      <th>WindSpeed</th>\n",
       "      <th>Pressure</th>\n",
       "      <th>PressureTrend</th>\n",
       "      <th>DewPoint</th>\n",
       "      <th>Humidity</th>\n",
       "      <th>Type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3002</td>\n",
       "      <td>BALTASOUND</td>\n",
       "      <td>15.0</td>\n",
       "      <td>60.749</td>\n",
       "      <td>-0.854</td>\n",
       "      <td>2018-05-31</td>\n",
       "      <td>1020</td>\n",
       "      <td>NaN</td>\n",
       "      <td>16.1</td>\n",
       "      <td>30000.0</td>\n",
       "      <td>E</td>\n",
       "      <td>8.0</td>\n",
       "      <td>1019.0</td>\n",
       "      <td>F</td>\n",
       "      <td>11.6</td>\n",
       "      <td>74.5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3002</td>\n",
       "      <td>BALTASOUND</td>\n",
       "      <td>15.0</td>\n",
       "      <td>60.749</td>\n",
       "      <td>-0.854</td>\n",
       "      <td>2018-05-31</td>\n",
       "      <td>1080</td>\n",
       "      <td>NaN</td>\n",
       "      <td>14.9</td>\n",
       "      <td>22000.0</td>\n",
       "      <td>E</td>\n",
       "      <td>8.0</td>\n",
       "      <td>1019.0</td>\n",
       "      <td>F</td>\n",
       "      <td>11.8</td>\n",
       "      <td>81.5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3002</td>\n",
       "      <td>BALTASOUND</td>\n",
       "      <td>15.0</td>\n",
       "      <td>60.749</td>\n",
       "      <td>-0.854</td>\n",
       "      <td>2018-05-31</td>\n",
       "      <td>1140</td>\n",
       "      <td>NaN</td>\n",
       "      <td>14.0</td>\n",
       "      <td>14000.0</td>\n",
       "      <td>E</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1018.0</td>\n",
       "      <td>F</td>\n",
       "      <td>11.6</td>\n",
       "      <td>85.4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3002</td>\n",
       "      <td>BALTASOUND</td>\n",
       "      <td>15.0</td>\n",
       "      <td>60.749</td>\n",
       "      <td>-0.854</td>\n",
       "      <td>2018-05-31</td>\n",
       "      <td>1200</td>\n",
       "      <td>NaN</td>\n",
       "      <td>12.9</td>\n",
       "      <td>12000.0</td>\n",
       "      <td>ENE</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1019.0</td>\n",
       "      <td>R</td>\n",
       "      <td>11.0</td>\n",
       "      <td>88.1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3002</td>\n",
       "      <td>BALTASOUND</td>\n",
       "      <td>15.0</td>\n",
       "      <td>60.749</td>\n",
       "      <td>-0.854</td>\n",
       "      <td>2018-05-31</td>\n",
       "      <td>1260</td>\n",
       "      <td>NaN</td>\n",
       "      <td>12.0</td>\n",
       "      <td>9000.0</td>\n",
       "      <td>E</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1019.0</td>\n",
       "      <td>R</td>\n",
       "      <td>10.9</td>\n",
       "      <td>92.9</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   StationID StationName  Elevation  Latitude  Longitude        Date  Time  \\\n",
       "0       3002  BALTASOUND       15.0    60.749     -0.854  2018-05-31  1020   \n",
       "1       3002  BALTASOUND       15.0    60.749     -0.854  2018-05-31  1080   \n",
       "2       3002  BALTASOUND       15.0    60.749     -0.854  2018-05-31  1140   \n",
       "3       3002  BALTASOUND       15.0    60.749     -0.854  2018-05-31  1200   \n",
       "4       3002  BALTASOUND       15.0    60.749     -0.854  2018-05-31  1260   \n",
       "\n",
       "   Gust  Temperature  Visibility WindDirection  WindSpeed  Pressure  \\\n",
       "0   NaN         16.1     30000.0             E        8.0    1019.0   \n",
       "1   NaN         14.9     22000.0             E        8.0    1019.0   \n",
       "2   NaN         14.0     14000.0             E        6.0    1018.0   \n",
       "3   NaN         12.9     12000.0           ENE        2.0    1019.0   \n",
       "4   NaN         12.0      9000.0             E        2.0    1019.0   \n",
       "\n",
       "  PressureTrend  DewPoint  Humidity  Type  \n",
       "0             F      11.6      74.5     0  \n",
       "1             F      11.8      81.5     0  \n",
       "2             F      11.6      85.4     0  \n",
       "3             R      11.0      88.1     0  \n",
       "4             R      10.9      92.9     1  "
      ]
     },
     "execution_count": 251,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load in the prepared weather data\n",
    "obs = pd.read_csv('weather.csv')\n",
    "obs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(106553, 17)"
      ]
     },
     "execution_count": 252,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>StationID</th>\n",
       "      <th>Elevation</th>\n",
       "      <th>Latitude</th>\n",
       "      <th>Longitude</th>\n",
       "      <th>Time</th>\n",
       "      <th>Gust</th>\n",
       "      <th>Temperature</th>\n",
       "      <th>Visibility</th>\n",
       "      <th>WindSpeed</th>\n",
       "      <th>Pressure</th>\n",
       "      <th>DewPoint</th>\n",
       "      <th>Humidity</th>\n",
       "      <th>Type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>106553.000000</td>\n",
       "      <td>106553.000000</td>\n",
       "      <td>106553.000000</td>\n",
       "      <td>106553.000000</td>\n",
       "      <td>106553.000000</td>\n",
       "      <td>7703.000000</td>\n",
       "      <td>106442.000000</td>\n",
       "      <td>92662.000000</td>\n",
       "      <td>102060.000000</td>\n",
       "      <td>99530.000000</td>\n",
       "      <td>106402.000000</td>\n",
       "      <td>106397.000000</td>\n",
       "      <td>106553.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>6147.845636</td>\n",
       "      <td>114.466594</td>\n",
       "      <td>53.673022</td>\n",
       "      <td>-2.829034</td>\n",
       "      <td>702.914418</td>\n",
       "      <td>33.043749</td>\n",
       "      <td>14.958912</td>\n",
       "      <td>25698.164404</td>\n",
       "      <td>8.999510</td>\n",
       "      <td>1018.748337</td>\n",
       "      <td>10.348008</td>\n",
       "      <td>76.185240</td>\n",
       "      <td>0.99603</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>15821.503845</td>\n",
       "      <td>171.669120</td>\n",
       "      <td>2.466079</td>\n",
       "      <td>2.269594</td>\n",
       "      <td>412.057262</td>\n",
       "      <td>13.424855</td>\n",
       "      <td>4.294516</td>\n",
       "      <td>14263.873943</td>\n",
       "      <td>6.087882</td>\n",
       "      <td>6.327468</td>\n",
       "      <td>3.120565</td>\n",
       "      <td>17.208653</td>\n",
       "      <td>0.92971</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>3002.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>49.207900</td>\n",
       "      <td>-10.250000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1.200000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>976.000000</td>\n",
       "      <td>-28.200000</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>3204.000000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>51.565000</td>\n",
       "      <td>-4.149000</td>\n",
       "      <td>360.000000</td>\n",
       "      <td>29.000000</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>14000.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>1015.000000</td>\n",
       "      <td>8.400000</td>\n",
       "      <td>65.300000</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>3414.000000</td>\n",
       "      <td>65.000000</td>\n",
       "      <td>53.175000</td>\n",
       "      <td>-2.663000</td>\n",
       "      <td>720.000000</td>\n",
       "      <td>32.000000</td>\n",
       "      <td>14.500000</td>\n",
       "      <td>25000.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>1018.000000</td>\n",
       "      <td>10.500000</td>\n",
       "      <td>79.200000</td>\n",
       "      <td>1.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>3769.000000</td>\n",
       "      <td>132.000000</td>\n",
       "      <td>55.285000</td>\n",
       "      <td>-1.097000</td>\n",
       "      <td>1020.000000</td>\n",
       "      <td>39.000000</td>\n",
       "      <td>17.500000</td>\n",
       "      <td>35000.000000</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>1024.000000</td>\n",
       "      <td>12.500000</td>\n",
       "      <td>90.000000</td>\n",
       "      <td>1.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>99142.000000</td>\n",
       "      <td>1245.000000</td>\n",
       "      <td>60.749000</td>\n",
       "      <td>1.348000</td>\n",
       "      <td>1380.000000</td>\n",
       "      <td>105.000000</td>\n",
       "      <td>31.600000</td>\n",
       "      <td>75000.000000</td>\n",
       "      <td>81.000000</td>\n",
       "      <td>1036.000000</td>\n",
       "      <td>22.700000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>3.00000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           StationID      Elevation       Latitude      Longitude  \\\n",
       "count  106553.000000  106553.000000  106553.000000  106553.000000   \n",
       "mean     6147.845636     114.466594      53.673022      -2.829034   \n",
       "std     15821.503845     171.669120       2.466079       2.269594   \n",
       "min      3002.000000       2.000000      49.207900     -10.250000   \n",
       "25%      3204.000000      20.000000      51.565000      -4.149000   \n",
       "50%      3414.000000      65.000000      53.175000      -2.663000   \n",
       "75%      3769.000000     132.000000      55.285000      -1.097000   \n",
       "max     99142.000000    1245.000000      60.749000       1.348000   \n",
       "\n",
       "                Time         Gust    Temperature    Visibility      WindSpeed  \\\n",
       "count  106553.000000  7703.000000  106442.000000  92662.000000  102060.000000   \n",
       "mean      702.914418    33.043749      14.958912  25698.164404       8.999510   \n",
       "std       412.057262    13.424855       4.294516  14263.873943       6.087882   \n",
       "min         0.000000     0.000000      -1.200000     20.000000       0.000000   \n",
       "25%       360.000000    29.000000      12.000000  14000.000000       5.000000   \n",
       "50%       720.000000    32.000000      14.500000  25000.000000       8.000000   \n",
       "75%      1020.000000    39.000000      17.500000  35000.000000      11.000000   \n",
       "max      1380.000000   105.000000      31.600000  75000.000000      81.000000   \n",
       "\n",
       "           Pressure       DewPoint       Humidity          Type  \n",
       "count  99530.000000  106402.000000  106397.000000  106553.00000  \n",
       "mean    1018.748337      10.348008      76.185240       0.99603  \n",
       "std        6.327468       3.120565      17.208653       0.92971  \n",
       "min      976.000000     -28.200000       0.800000       0.00000  \n",
       "25%     1015.000000       8.400000      65.300000       0.00000  \n",
       "50%     1018.000000      10.500000      79.200000       1.00000  \n",
       "75%     1024.000000      12.500000      90.000000       1.00000  \n",
       "max     1036.000000      22.700000     100.000000       3.00000  "
      ]
     },
     "execution_count": 253,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obs.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this exercise we will use **8** input features (provided) and clean the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define 8 input feature variables, 1 target variable data, and names of the 3 weather types\n",
    "features = ['Latitude', 'Elevation', 'Temperature', 'Visibility', 'WindSpeed', 'Pressure', 'Humidity', 'WindDirection']\n",
    "output   = ['Type']\n",
    "wtype    = ['Clear', 'Cloudy', 'Precip']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define derived dataset containing only the relevant columns and rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(86313, 9)"
      ]
     },
     "execution_count": 255,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reduce to feature and type columns\n",
    "dataset = obs[features + output]\n",
    "\n",
    "# Drop duplicates and null values \n",
    "dataset = dataset.drop_duplicates().dropna()\n",
    "\n",
    "# Drop unrecorded weather type\n",
    "dataset = dataset[dataset.Type != 3]\n",
    "\n",
    "# Check shape \n",
    "dataset.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Conceptual questions (2.5 Marks)\n",
    "---\n",
    "This section covers **5** exercises on conceptual understanding of neural networks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Which are the most used activation functions and why do we (typically) need non-linear activation functions in neural networks? (0.5 mark)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most used activation functions are:\n",
    "-  sigmoid $f(a) = \\frac{1}{1+e^{a}}$\n",
    "-  tanh $f(a) = tanh(a)$\n",
    "-  softplus $f(a) = ln(1+e^{a})$\n",
    "-  Relu $f(a) = max(0, a)$\n",
    "\n",
    "Without non-linear activation functions, regardless of how many layers and nodes we use, the model will still be a linear fit. Therefore, non-linear activation functions add complexity to the model so we can model non-linear input data. Gradient descent cannot be used to train models using linear activation functions because there the derivative of the function is independent of the input so the weights cannot be learned."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Why do we need deep neural networks and which are the main differences between deep and shallow learning? (0.5 Mark)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deep neural networks can not only make predictions on unseen inputs but actually understand basic features of the data. It extracts relevant features itself and does not require manual feature engineering like shallow learning which means it can learn from high dimentional data unlike shallow leanring where preprocessing is required to lower the dimensionality of data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Discuss the Bias-variance trade-off and its relation to underfitting and overfitting of a model. Which are the caractheristics of an ideal model?  (0.5 mark)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bias and variance are two sources of error in the model. High bias refers to constraining the model based on assumptions between how input and output are related. High variance refers to oversensitivity to random fluctuations in the training data resulting in poor predictions on the test data.\n",
    "\n",
    "1. Underfitting, for example fitting a straight line to data that is quadratically distributed. Here, there is high bias as we have constrained the output to a function too simple to reflect the complexity of the input data. However, there is low variance, as the simplicity means it does not look closely enough mistake random noise for features.\n",
    "1. Overfitting, for example fitting a quartic to data that is quadratically distributed. Here, there is low bias as the model has enough free parameters to capure the complexity of the input. However, there is high variance as random noise from the training data is also captured like a feature so error arises in testing as the random noise in the testing data is different from the training data.\n",
    "\n",
    "An ideal model is one with enough complexity to capture the features and not be underfitted, but not so much that it cannot be generalised to apply to the test data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Given a neural network with 4 input nodes, 2 layers with 5 nodes each, and 1 output node, what is the total number of free (trainable) parameters in the network? Does it matter which activation function(s) are used?  (0.5 mark)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "61 trainable parameters including both weights and biases. The choice of activation functions does not change the number of trainable parameters as it only depends on the network architecture."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. What are appropriate choice for _(a)_ the number of output nodes and _(b)_ output activation function(s) for each of the following tasks, and why? (0.5 mark)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Regression of the $x$, $y$, and $z$ coordinates of a single particle in an arbitrary coordinate system\n",
    "2. Regression of particle energy of a single particle\n",
    "3. Classification of two processes (signal vs. background)\n",
    "4. Classification among *N* classes (dog vs. cat vs. fish vs. ...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. a) 3 output nodes, one for each coordinate b) Linear activation function as we would like coordinates to range from $\\pm \\infty$: $f(a) = ma$ where $m$ is a constant\n",
    "1. a) 1 output node b) Relu, to make sure energy is always positive\n",
    "1. a) 1 output node b) Sigmoid\n",
    "1. a) N output nodes, one probability score for each class b) Softmax so result can be interpreted as probability distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data preprocessing and RTs (2.5 mark)\n",
    "---\n",
    "This section covers **4** exercises on data preparation, feature standardisation, and dataset splitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Relevant import(s) for this section\n",
    "from sklearn import tree\n",
    "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
    "from sklearn.tree import export_graphviz\n",
    "from sklearn.model_selection import train_test_split # Import train_test_split function\n",
    "from sklearn import metrics # Import scikit-learn metrics module for accuracy calculation\n",
    "from sklearn import preprocessing # Import preprocessing for String-Int conversion\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler, MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "**_Comment on target format and one-hot encoding:_** By default, the target column (`Type`) contains one integer (0, 1, or 2) for each example, the integer specifying one of three possible types of weather. However, for doing multi-class classification (which this is), we want our neural network to have one output node per class (_i.e._ 3 output nodes in this case), such that the activation of each output node is interpreted as the likelihood for a given sample being of the type in question. Therefore, the target should also be a 3-element vector for each sample; this vector should be all zeros, except for a $1$ at the index corresponding to the type in question. This is called **one-hot encoding**, and a few examples are shown below:\n",
    "\n",
    "- type = 0 $\\to$ one-hot = $[1, 0, 0]$ for 3 classes\n",
    "- type = 1 $\\to$ one-hot = $[0, 1, 0]$ for 3 classes\n",
    "- type = 2 $\\to$ one-hot = $[0, 0, 1]$ for 3 classes\n",
    "\n",
    "This is the target towards which a neural network classifier is trained: That is, ideally, for an example of type 0, the network will output a large activation ($\\approx 1$) on the first output node (interpreted as a large likelihood for the first weather type), and very small activations ($\\ll 1$) on the two other output nodes (intepreted as small likelihoods for the two other weather types); and so on.\n",
    "\n",
    "The same type of one-hot encoding can be performed for any number of target classes $N_{c}$, which just results in $N_{c}$-element target vectors with a single non-zero entry each.\n",
    "\n",
    "To be user friendly, however, `scikit-learn` allows us to use integer targets for multi-class classification — it does the one-hot encoding for us \"under the hood.\" Similarly, `keras`, _can_ also allow us to use integer targets for multi-class classification, provided we use the appropropriate loss (`sparse_categorical_crossentropy`). Otherwise (if we use `categorical_crossentropy` loss), it expects one-hot encoded targets. Which approach you choose is up to you — but now you know what goes on.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6. Prepare the feature and target arrays (0.5 mark)\n",
    "- Randomly sample **3,500** observations per weather type (**10,500** observations in total) from `dataset` into a new `pandas.DataFrame`; call it `sample`.\n",
    "- One-hot encode the **wind direction** variable (_i.e._ $N$ to $[1, 0, \\ldots, 0]$, $NNE$ to $[0, 1, \\ldots, 0]$, _etc._ ), to allow us to input it to the neural network. The exact order of the encoding (_i.e._ which direction corresponds to which index) doesn't matter. *Hint:*\n",
    "  - *Either:* Use the scikit-learn `ColumnTransformer` with the `OneHotEncoder` applied to the `WindDirection` column, and let the remainder of the features pass through un-transformed.\n",
    "  - *Or:* Use the `OneHotEncoder` class directly on the `WindDirection` column (use `sparse=False` in the `OneHotEncoder` constructor), and then concatenate with a `numpy.array` containing the remaining features.\n",
    "- Define `numpy.arrays` named `X` and `y` containing the training features (the 7 unmodified ones plus the one-hot encoded wind directions) and target, respectively.\n",
    "- Argue whether the shapes of `X` and `y` are as expected/as they should be."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>Latitude</th>\n",
       "      <th>Elevation</th>\n",
       "      <th>Temperature</th>\n",
       "      <th>Visibility</th>\n",
       "      <th>WindSpeed</th>\n",
       "      <th>Pressure</th>\n",
       "      <th>Humidity</th>\n",
       "      <th>Type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>60.749</td>\n",
       "      <td>15.0</td>\n",
       "      <td>16.1</td>\n",
       "      <td>30000.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>1019.0</td>\n",
       "      <td>74.5</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>60.749</td>\n",
       "      <td>15.0</td>\n",
       "      <td>14.9</td>\n",
       "      <td>22000.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>1019.0</td>\n",
       "      <td>81.5</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>60.749</td>\n",
       "      <td>15.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>14000.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1018.0</td>\n",
       "      <td>85.4</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>60.749</td>\n",
       "      <td>15.0</td>\n",
       "      <td>12.9</td>\n",
       "      <td>12000.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1019.0</td>\n",
       "      <td>88.1</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>60.749</td>\n",
       "      <td>15.0</td>\n",
       "      <td>15.3</td>\n",
       "      <td>35000.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1018.0</td>\n",
       "      <td>87.2</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     0    1    2    3    4    5    6    7    8    9  ...   14   15  Latitude  \\\n",
       "0  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0    60.749   \n",
       "1  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0    60.749   \n",
       "2  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0    60.749   \n",
       "3  0.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0    60.749   \n",
       "4  0.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0    60.749   \n",
       "\n",
       "   Elevation  Temperature  Visibility  WindSpeed  Pressure  Humidity  Type  \n",
       "0       15.0         16.1     30000.0        8.0    1019.0      74.5   0.0  \n",
       "1       15.0         14.9     22000.0        8.0    1019.0      81.5   0.0  \n",
       "2       15.0         14.0     14000.0        6.0    1018.0      85.4   0.0  \n",
       "3       15.0         12.9     12000.0        2.0    1019.0      88.1   0.0  \n",
       "4       15.0         15.3     35000.0        7.0    1018.0      87.2   0.0  \n",
       "\n",
       "[5 rows x 24 columns]"
      ]
     },
     "execution_count": 257,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get first 3500 observations for each class\n",
    "frames = [dataset[dataset.Type == 0].head(3500), \n",
    "          dataset[dataset.Type == 1].head(3500),\n",
    "          dataset[dataset.Type == 2].head(3500)]\n",
    "\n",
    "# Concat into one dataframe\n",
    "sample = pd.concat(frames)\n",
    "sample.head()\n",
    "\n",
    "#features = ['Latitude', 'Elevation', 'Temperature', 'Visibility', 'WindSpeed', 'Pressure', 'Humidity', 'WindDirection']\n",
    "#target = ['Type']\n",
    "\n",
    "ct = ColumnTransformer([(\"one_hot_encoder\", OneHotEncoder(), ['WindDirection'])], remainder='passthrough')\n",
    "sample = pd.DataFrame(ct.fit_transform(sample))\n",
    "sample.columns = list(range(16)) + features[:-1] + output\n",
    "sample.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((10500, 23), (10500,))"
      ]
     },
     "execution_count": 258,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = sample.iloc[:, :-1].values\n",
    "y = sample.iloc[:, -1].values.flatten()\n",
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'# Get first 3500 observations for each class\\nframes = [dataset[dataset.Type == 0].head(3500), \\n          dataset[dataset.Type == 1].head(3500),\\n          dataset[dataset.Type == 2].head(3500)]\\n\\n# Concat into one dataframe\\nsample = pd.concat(frames)\\nsample.head()\\n\\nfeatures = [\\'Latitude\\', \\'Elevation\\', \\'Temperature\\', \\'Visibility\\', \\'WindSpeed\\', \\'Pressure\\', \\'Humidity\\', \\'WindDirection\\']\\ntarget = [\\'Type\\']\\n\\nct = ColumnTransformer([(\"one_hot_encoder\", OneHotEncoder(), [\\'WindDirection\\'])], remainder=\\'passthrough\\')\\nsample = pd.DataFrame(ct.fit_transform(sample))\\nsample[\\'WindDirection\\'] = sample.iloc[:, :16].values.tolist()\\nsample = sample.drop(sample.columns[:16], axis=1)\\ncols = sample.columns.tolist()\\ncols = cols[:7] + [cols[8]] + [cols[7]]\\nsample = sample[cols]\\nsample.columns = features + target\\nsample.head()'"
      ]
     },
     "execution_count": 259,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"# Get first 3500 observations for each class\n",
    "frames = [dataset[dataset.Type == 0].head(3500), \n",
    "          dataset[dataset.Type == 1].head(3500),\n",
    "          dataset[dataset.Type == 2].head(3500)]\n",
    "\n",
    "# Concat into one dataframe\n",
    "sample = pd.concat(frames)\n",
    "sample.head()\n",
    "\n",
    "features = ['Latitude', 'Elevation', 'Temperature', 'Visibility', 'WindSpeed', 'Pressure', 'Humidity', 'WindDirection']\n",
    "target = ['Type']\n",
    "\n",
    "ct = ColumnTransformer([(\"one_hot_encoder\", OneHotEncoder(), ['WindDirection'])], remainder='passthrough')\n",
    "sample = pd.DataFrame(ct.fit_transform(sample))\n",
    "sample['WindDirection'] = sample.iloc[:, :16].values.tolist()\n",
    "sample = sample.drop(sample.columns[:16], axis=1)\n",
    "cols = sample.columns.tolist()\n",
    "cols = cols[:7] + [cols[8]] + [cols[7]]\n",
    "sample = sample[cols]\n",
    "sample.columns = features + target\n",
    "sample.head()\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'X = sample[features].values\\ny = sample[target].values.flatten()\\nX.shape, y.shape'"
      ]
     },
     "execution_count": 260,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"X = sample[features].values\n",
    "y = sample[target].values.flatten()\n",
    "X.shape, y.shape\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7. Train a Random Forest and evaluate the performance (1 mark)\n",
    "\n",
    "Decision trees work well with a mixture of features (of different scales, and bot binary and continuos), so we will train a random forest to do the job of categorisation.\n",
    "\n",
    "You are given the train test split:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7350, 23) (3150, 23)\n"
     ]
    }
   ],
   "source": [
    "#Import random fosets and confusion matrix metric\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# split dataset into training set and test set\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1) # 70% training and 30% test\n",
    "print(x_train.shape,x_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Train a RandomForestClassifier with 1000 estimators, `gini` seperation criteria, and max depth 4.\n",
    "2. Check the overal accuaracy on the testing set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Use the confusion_matrix method to return the confusion matrix normalised over the true lables, i.e. sum over rows should sum to 100%. Use the given colormap to plot the confusion matrix in a heatmap.\n",
    "    - Define the axis tick names to represent Clear, Cloudy or Precip\n",
    "    - Use suitable x and y axis labels\n",
    "    \n",
    "4. What are the true positive rates for clear, cloudy and perp? What is the probability that rain is forcast given that the day is clear?\n",
    "5. Which features does the random forest deem the most important?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Training: 0.7531972789115646\n",
      "Accuracy Testing: 0.7590476190476191\n"
     ]
    }
   ],
   "source": [
    "# Fit a random forest classifier\n",
    "rf = RandomForestClassifier(n_estimators=1000, criterion = 'gini', max_depth=4,random_state=1)\n",
    "rf.fit(x_train, y_train)\n",
    "y_pred = rf.predict(x_train)\n",
    "print(\"Accuracy Training:\",metrics.accuracy_score(y_train, y_pred))\n",
    "y_pred = rf.predict(x_test)\n",
    "print(\"Accuracy Testing:\",metrics.accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "conf_matrix = confusion_matrix(y_test, y_pred, normalize='true')\n",
    "\n",
    "colormap = sns.diverging_palette(220, 10, as_cmap=True)\n",
    "fig, ax = plt.subplots(figsize=(10, 10))\n",
    "# Generate Heat Map, allow annotations and place floats in map\n",
    "sns.heatmap(conf_matrix, cmap=plt.cm.Blues, annot=True, fmt=\".2f\")\n",
    "ax.set_ylabel('True Weather');\n",
    "ax.set_xlabel('Predicted Weather');\n",
    "ax.set_xticklabels(wtype);\n",
    "ax.set_yticklabels(wtype);\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True positive rate for Clear is 0.8607954545454546\n",
      "True positive rate for Cloudy is 0.7497603068072867\n",
      "True positive rate for Precip is 0.6660323501427212\n",
      "\n",
      "Probability of predicting rain, given true weather is clear, is 0.010416666666666666\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(wtype)):\n",
    "    print('True positive rate for ' + wtype[i] + ' is ' + str(conf_matrix[i][i]))\n",
    "print('')\n",
    "print('Probability of predicting rain, given true weather is clear, is ' + str(conf_matrix[0][2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([<matplotlib.axis.YTick at 0x231c53b2f88>,\n",
       "  <matplotlib.axis.YTick at 0x231bee75208>,\n",
       "  <matplotlib.axis.YTick at 0x231c144dbc8>,\n",
       "  <matplotlib.axis.YTick at 0x231cbdceac8>,\n",
       "  <matplotlib.axis.YTick at 0x231c6e20e48>,\n",
       "  <matplotlib.axis.YTick at 0x231c74f2f88>,\n",
       "  <matplotlib.axis.YTick at 0x231ca055948>,\n",
       "  <matplotlib.axis.YTick at 0x231c74fc348>,\n",
       "  <matplotlib.axis.YTick at 0x231c74fc048>,\n",
       "  <matplotlib.axis.YTick at 0x231cbdb4d88>,\n",
       "  <matplotlib.axis.YTick at 0x231cbda3d08>,\n",
       "  <matplotlib.axis.YTick at 0x231cbd99848>,\n",
       "  <matplotlib.axis.YTick at 0x231ca05b188>,\n",
       "  <matplotlib.axis.YTick at 0x231cbd93988>,\n",
       "  <matplotlib.axis.YTick at 0x231cbdaa108>,\n",
       "  <matplotlib.axis.YTick at 0x231cbdb8a48>,\n",
       "  <matplotlib.axis.YTick at 0x231c6e014c8>,\n",
       "  <matplotlib.axis.YTick at 0x231cbdbca88>,\n",
       "  <matplotlib.axis.YTick at 0x231cbdbcbc8>,\n",
       "  <matplotlib.axis.YTick at 0x231c74db208>,\n",
       "  <matplotlib.axis.YTick at 0x231c74cb688>,\n",
       "  <matplotlib.axis.YTick at 0x231c6dfc488>,\n",
       "  <matplotlib.axis.YTick at 0x231cbdbcf08>],\n",
       " [Text(0, 0, '0'),\n",
       "  Text(0, 1, '1'),\n",
       "  Text(0, 2, '2'),\n",
       "  Text(0, 3, '3'),\n",
       "  Text(0, 4, '4'),\n",
       "  Text(0, 5, '5'),\n",
       "  Text(0, 6, '6'),\n",
       "  Text(0, 7, '7'),\n",
       "  Text(0, 8, '8'),\n",
       "  Text(0, 9, '9'),\n",
       "  Text(0, 10, '10'),\n",
       "  Text(0, 11, '11'),\n",
       "  Text(0, 12, '12'),\n",
       "  Text(0, 13, '13'),\n",
       "  Text(0, 14, '14'),\n",
       "  Text(0, 15, '15'),\n",
       "  Text(0, 16, 'Latitude'),\n",
       "  Text(0, 17, 'Elevation'),\n",
       "  Text(0, 18, 'Temperature'),\n",
       "  Text(0, 19, 'Visibility'),\n",
       "  Text(0, 20, 'WindSpeed'),\n",
       "  Text(0, 21, 'Pressure'),\n",
       "  Text(0, 22, 'Humidity')])"
      ]
     },
     "execution_count": 265,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Given plotting example for feature importance\n",
    "plt.barh(range(23), rf.feature_importances_)\n",
    "plt.yticks(range(23), list(range(16))+features[:-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8. Standardise the relevant features (0.5 mark)\n",
    "\n",
    "_Note:_ You shouldn't standardise the one-hot encoded wind directions; they already have the desired format. Perform a sanity check to make sure that the resulting features have the expected distributional properties (mean and standard deviation; or minimum and maximum value).\n",
    "- Hint:\n",
    "\n",
    "    - Use the scikit-learn `StandardScaler`\n",
    "    - Or use the scikit-learn `MinMaxScaler`\n",
    "\n",
    "- Perform a sanity check to make sure that the resulting features have the expected distributional properties (mean and standard deviation; or minimum and maximum value).\n",
    "    - The number of columns should match, and depending on the choice of standardisation, the last 7 columns should either have:\n",
    "      - (Using `StandardScaler`) means = 0 and standard deviations = 1; or\n",
    "      - (Using `MinMaxScaler`) min = 0, max = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Matthew\\Anaconda3\\envs\\daml\\lib\\site-packages\\sklearn\\utils\\validation.py:1677: FutureWarning: Feature names only support names that are all strings. Got feature names with dtypes: ['int', 'str']. An error will be raised in 1.2.\n",
      "  FutureWarning,\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Latitude</th>\n",
       "      <th>Elevation</th>\n",
       "      <th>Temperature</th>\n",
       "      <th>Visibility</th>\n",
       "      <th>WindSpeed</th>\n",
       "      <th>Pressure</th>\n",
       "      <th>Humidity</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>...</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>Type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1.050000e+04</td>\n",
       "      <td>1.050000e+04</td>\n",
       "      <td>1.050000e+04</td>\n",
       "      <td>1.050000e+04</td>\n",
       "      <td>1.050000e+04</td>\n",
       "      <td>1.050000e+04</td>\n",
       "      <td>1.050000e+04</td>\n",
       "      <td>10500.000000</td>\n",
       "      <td>10500.000000</td>\n",
       "      <td>10500.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>10500.000000</td>\n",
       "      <td>10500.000000</td>\n",
       "      <td>10500.000000</td>\n",
       "      <td>10500.000000</td>\n",
       "      <td>10500.000000</td>\n",
       "      <td>10500.000000</td>\n",
       "      <td>10500.000000</td>\n",
       "      <td>10500.000000</td>\n",
       "      <td>10500.000000</td>\n",
       "      <td>10500.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>2.045280e-13</td>\n",
       "      <td>-4.330927e-17</td>\n",
       "      <td>-9.917823e-15</td>\n",
       "      <td>-7.579123e-17</td>\n",
       "      <td>-8.661854e-17</td>\n",
       "      <td>-2.338701e-15</td>\n",
       "      <td>-4.304942e-14</td>\n",
       "      <td>0.078190</td>\n",
       "      <td>0.070571</td>\n",
       "      <td>0.043524</td>\n",
       "      <td>...</td>\n",
       "      <td>0.040762</td>\n",
       "      <td>0.091524</td>\n",
       "      <td>0.042571</td>\n",
       "      <td>0.049048</td>\n",
       "      <td>0.078762</td>\n",
       "      <td>0.071429</td>\n",
       "      <td>0.079905</td>\n",
       "      <td>0.039524</td>\n",
       "      <td>0.067143</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.000048e+00</td>\n",
       "      <td>1.000048e+00</td>\n",
       "      <td>1.000048e+00</td>\n",
       "      <td>1.000048e+00</td>\n",
       "      <td>1.000048e+00</td>\n",
       "      <td>1.000048e+00</td>\n",
       "      <td>1.000048e+00</td>\n",
       "      <td>0.268484</td>\n",
       "      <td>0.256120</td>\n",
       "      <td>0.204043</td>\n",
       "      <td>...</td>\n",
       "      <td>0.197748</td>\n",
       "      <td>0.288366</td>\n",
       "      <td>0.201898</td>\n",
       "      <td>0.215978</td>\n",
       "      <td>0.269380</td>\n",
       "      <td>0.257552</td>\n",
       "      <td>0.271159</td>\n",
       "      <td>0.194847</td>\n",
       "      <td>0.250281</td>\n",
       "      <td>0.816535</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-1.886305e+00</td>\n",
       "      <td>-1.029889e+00</td>\n",
       "      <td>-3.648023e+00</td>\n",
       "      <td>-1.213670e+00</td>\n",
       "      <td>-1.303933e+00</td>\n",
       "      <td>-7.475503e+00</td>\n",
       "      <td>-4.002378e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>-8.601435e-01</td>\n",
       "      <td>-8.210833e-01</td>\n",
       "      <td>-7.002490e-01</td>\n",
       "      <td>-8.295090e-01</td>\n",
       "      <td>-7.921979e-01</td>\n",
       "      <td>-6.320814e-02</td>\n",
       "      <td>-5.664767e-01</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>-6.332517e-02</td>\n",
       "      <td>-3.222686e-01</td>\n",
       "      <td>-9.335442e-02</td>\n",
       "      <td>-2.895243e-01</td>\n",
       "      <td>-1.098849e-01</td>\n",
       "      <td>3.169095e-01</td>\n",
       "      <td>2.868350e-01</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>6.558707e-01</td>\n",
       "      <td>4.897554e-01</td>\n",
       "      <td>6.291391e-01</td>\n",
       "      <td>6.361636e-01</td>\n",
       "      <td>4.018499e-01</td>\n",
       "      <td>5.069684e-01</td>\n",
       "      <td>8.154352e-01</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>2.684352e+00</td>\n",
       "      <td>3.123033e+00</td>\n",
       "      <td>3.374615e+00</td>\n",
       "      <td>4.570338e+00</td>\n",
       "      <td>6.883823e+00</td>\n",
       "      <td>2.027439e+00</td>\n",
       "      <td>1.102390e+00</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           Latitude     Elevation   Temperature    Visibility     WindSpeed  \\\n",
       "count  1.050000e+04  1.050000e+04  1.050000e+04  1.050000e+04  1.050000e+04   \n",
       "mean   2.045280e-13 -4.330927e-17 -9.917823e-15 -7.579123e-17 -8.661854e-17   \n",
       "std    1.000048e+00  1.000048e+00  1.000048e+00  1.000048e+00  1.000048e+00   \n",
       "min   -1.886305e+00 -1.029889e+00 -3.648023e+00 -1.213670e+00 -1.303933e+00   \n",
       "25%   -8.601435e-01 -8.210833e-01 -7.002490e-01 -8.295090e-01 -7.921979e-01   \n",
       "50%   -6.332517e-02 -3.222686e-01 -9.335442e-02 -2.895243e-01 -1.098849e-01   \n",
       "75%    6.558707e-01  4.897554e-01  6.291391e-01  6.361636e-01  4.018499e-01   \n",
       "max    2.684352e+00  3.123033e+00  3.374615e+00  4.570338e+00  6.883823e+00   \n",
       "\n",
       "           Pressure      Humidity             0             1             2  \\\n",
       "count  1.050000e+04  1.050000e+04  10500.000000  10500.000000  10500.000000   \n",
       "mean  -2.338701e-15 -4.304942e-14      0.078190      0.070571      0.043524   \n",
       "std    1.000048e+00  1.000048e+00      0.268484      0.256120      0.204043   \n",
       "min   -7.475503e+00 -4.002378e+00      0.000000      0.000000      0.000000   \n",
       "25%   -6.320814e-02 -5.664767e-01      0.000000      0.000000      0.000000   \n",
       "50%    3.169095e-01  2.868350e-01      0.000000      0.000000      0.000000   \n",
       "75%    5.069684e-01  8.154352e-01      0.000000      0.000000      0.000000   \n",
       "max    2.027439e+00  1.102390e+00      1.000000      1.000000      1.000000   \n",
       "\n",
       "       ...             7             8             9            10  \\\n",
       "count  ...  10500.000000  10500.000000  10500.000000  10500.000000   \n",
       "mean   ...      0.040762      0.091524      0.042571      0.049048   \n",
       "std    ...      0.197748      0.288366      0.201898      0.215978   \n",
       "min    ...      0.000000      0.000000      0.000000      0.000000   \n",
       "25%    ...      0.000000      0.000000      0.000000      0.000000   \n",
       "50%    ...      0.000000      0.000000      0.000000      0.000000   \n",
       "75%    ...      0.000000      0.000000      0.000000      0.000000   \n",
       "max    ...      1.000000      1.000000      1.000000      1.000000   \n",
       "\n",
       "                 11            12            13            14            15  \\\n",
       "count  10500.000000  10500.000000  10500.000000  10500.000000  10500.000000   \n",
       "mean       0.078762      0.071429      0.079905      0.039524      0.067143   \n",
       "std        0.269380      0.257552      0.271159      0.194847      0.250281   \n",
       "min        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "25%        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "50%        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "75%        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "max        1.000000      1.000000      1.000000      1.000000      1.000000   \n",
       "\n",
       "               Type  \n",
       "count  10500.000000  \n",
       "mean       1.000000  \n",
       "std        0.816535  \n",
       "min        0.000000  \n",
       "25%        0.000000  \n",
       "50%        1.000000  \n",
       "75%        2.000000  \n",
       "max        2.000000  \n",
       "\n",
       "[8 rows x 24 columns]"
      ]
     },
     "execution_count": 266,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ct = ColumnTransformer([(\"scale\", StandardScaler(), features[:-1])], remainder='passthrough')\n",
    "sample_scaled = pd.DataFrame(ct.fit_transform(sample))\n",
    "sample_scaled.columns = features[:-1] + list(range(16)) + output\n",
    "sample_scaled.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = sample_scaled.iloc[:, :-1].values\n",
    "y = sample_scaled.iloc[:, -1].values.flatten()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 9. Split the dataset into a training and a testing part (0.5)\n",
    "\n",
    "Reserve **30%** of data for testing. Check whether the resulting arrays have the expected shapes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7350, 23) (3150, 23) (7350,) (3150,)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1)\n",
    "print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Neural networks in `scikit-learn` (1.5 mark)\n",
    "---\n",
    "This section covers **2** exercises on constructing and training neural networks using the `scikit-learn` library, as well as evaluating neural network performance. `scikit-learn` provide many, very easy to use ML algorithms, including neural networks. These are called `MLPClassifier` (MLP = multi-layer perceptron; a historic name for densely connected, feed-forward neural networks) when used for classification, and `MLPRegressor` when used for regression. We will focus on the former for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Relevant import(s) for this section\n",
    "from sklearn.neural_network import MLPClassifier\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 10. Construct and train a neural network  (1 mark)\n",
    "\n",
    "- Create an `MLPClassifier` which\n",
    "    - has **1 hidden layer of 50 neurons** \n",
    "    - has **no regularization term**\n",
    "    - trains for a maximum of **100 epochs** \n",
    "    - uses a batch size of **32**\n",
    "- Fit the classifier using the standard `.fit()` member method.\n",
    "- Plot the loss function value as a function of number of epochs (0.5 of mark).\n",
    "  You can access the loss history through the `.loss_curve_` attribute of the `MLPClassifier` instance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLPClassifier(alpha=0, batch_size=32, hidden_layer_sizes=(50,), max_iter=100)"
      ]
     },
     "execution_count": 270,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = MLPClassifier(max_iter=100, alpha=0, hidden_layer_sizes=(50,), batch_size=32)\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(range(len(clf.loss_curve_)), clf.loss_curve_, '-ok')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 11. Performance evaluation (0.5 mark)\n",
    "\n",
    "- Using the testing dataset: \n",
    "    - Compute the overall accuracy for the classifier using the `MLPClassifier`'s `.score()` member method for both testing and training datasets.\n",
    "    - Compute the confusion matrix (normalised in true labels), and plot it \n",
    "- Discuss the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Training: 0.8668027210884354\n",
      "Accuracy Testing: 0.8238095238095238\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy Training:\",clf.score(X_train, y_train))\n",
    "print(\"Accuracy Testing:\",clf.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Neural networks in `Keras` (2 marks)\n",
    "---\n",
    "This section covers **2** exercises on constructing and training neural networks using the `Keras` library. `scikit-learn` is very easy to use, but libraries like `Keras` provide a lot more flexibility, which is why we will be using these extensively in the last two units of the _'Data science tools and machine learning'_ track."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Relevant import(s) for this section\n",
    "from tensorflow.python.keras.models import Model\n",
    "from tensorflow.python.keras.layers import Input, Dense"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 12. Construct a neural network in `Keras` (1 mark)\n",
    "\n",
    "- Create a `keras.Model` using the **Keras functional API**. The network should have:\n",
    "    - An input layer with the same number of nodes as the number of features in `X`.\n",
    "    - A single, densely connected hidden layer with **50 nodes** equipped with **ReLU activation**.\n",
    "    - A densely connected output layer with **3 nodes** (the number of types of weather we're classifying) equipped with **softmax activation**.\n",
    "- Compile the model the using the **Adam optimiser**, add `'accuracy'` as metric, and use either:\n",
    "    - `categorical_crossentropy` loss, if you have one-hot encoded the targets `y`, or\n",
    "    - `sparse_categorical_crossentropy` loss if you hare using integer-valued targets.\n",
    "- Use the `.summary()` member method to print an overview of the model you have created, explain the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_6\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_7 (InputLayer)         [(None, 23)]              0         \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 50)                1200      \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 3)                 153       \n",
      "=================================================================\n",
      "Total params: 1,353\n",
      "Trainable params: 1,353\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "input = Input(shape=(len(X_train[0]),))\n",
    "h = Dense(50, activation='relu')(input)\n",
    "output = Dense(3, activation='softmax')(h)\n",
    "model = Model(input, output)\n",
    "model.compile('adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first column displays the input, hidden (dense) and output (dense) layers as were defined. The middle column shows (None, 50) meaning the hidden layer accepts sequences of any batch size in length but fixed 50 dimensional vectors corresponding to the 50 nodes in that layer. The last column is the number of trainable parameters by layer, for the output layer this is the $3 \\times 50$ weights plus $3$ biases $= 150$. Finally, also displayed is the total number of trainable parameters of all layers. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 13. Train a `Keras` neural network (1 mark)\n",
    "\n",
    "- Use the `.fit()` member method to train the network on the **training dataset** for **100 epochs** with a **batch size of 32**. Use **20% of the data for validation** and make sure to have `Keras` **shuffle** the training data between epochs. Save the fit history by doing `history_mld = .....`\n",
    "- Print the classification accuracy using the `.evaluate()` member method, for both the training and testing dataset. Comment on the results.\n",
    "- Plot val_loss and loss functions from the fit history. On the same plot, plot the sklearn curve from the excercise above. Note the sklearn NN does not provide a complementary validation loss history, so only plot the training loss.\n",
    "- Comment on the results of the overall accuracy compared to the scikit-learn method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 5880 samples, validate on 1470 samples\n",
      "Epoch 1/100\n",
      "5880/5880 [==============================] - 0s 56us/sample - loss: 0.8057 - acc: 0.6459 - val_loss: 0.6514 - val_acc: 0.7456\n",
      "Epoch 2/100\n",
      "5880/5880 [==============================] - 0s 43us/sample - loss: 0.5990 - acc: 0.7675 - val_loss: 0.5844 - val_acc: 0.7667\n",
      "Epoch 3/100\n",
      "5880/5880 [==============================] - 0s 40us/sample - loss: 0.5518 - acc: 0.7806 - val_loss: 0.5547 - val_acc: 0.7755\n",
      "Epoch 4/100\n",
      "5880/5880 [==============================] - 0s 39us/sample - loss: 0.5280 - acc: 0.7927 - val_loss: 0.5321 - val_acc: 0.7830\n",
      "Epoch 5/100\n",
      "5880/5880 [==============================] - 0s 46us/sample - loss: 0.5112 - acc: 0.8009 - val_loss: 0.5201 - val_acc: 0.7959\n",
      "Epoch 6/100\n",
      "5880/5880 [==============================] - 0s 42us/sample - loss: 0.4979 - acc: 0.8044 - val_loss: 0.5065 - val_acc: 0.8000\n",
      "Epoch 7/100\n",
      "5880/5880 [==============================] - 0s 46us/sample - loss: 0.4870 - acc: 0.8099 - val_loss: 0.4955 - val_acc: 0.8007\n",
      "Epoch 8/100\n",
      "5880/5880 [==============================] - 0s 41us/sample - loss: 0.4780 - acc: 0.8145 - val_loss: 0.4905 - val_acc: 0.7986\n",
      "Epoch 9/100\n",
      "5880/5880 [==============================] - 0s 45us/sample - loss: 0.4704 - acc: 0.8126 - val_loss: 0.4804 - val_acc: 0.7993\n",
      "Epoch 10/100\n",
      "5880/5880 [==============================] - 0s 34us/sample - loss: 0.4624 - acc: 0.8179 - val_loss: 0.4760 - val_acc: 0.8041\n",
      "Epoch 11/100\n",
      "5880/5880 [==============================] - 0s 51us/sample - loss: 0.4565 - acc: 0.8209 - val_loss: 0.4722 - val_acc: 0.8000\n",
      "Epoch 12/100\n",
      "5880/5880 [==============================] - 0s 43us/sample - loss: 0.4508 - acc: 0.8233 - val_loss: 0.4672 - val_acc: 0.8048\n",
      "Epoch 13/100\n",
      "5880/5880 [==============================] - 0s 37us/sample - loss: 0.4459 - acc: 0.8240 - val_loss: 0.4654 - val_acc: 0.8041\n",
      "Epoch 14/100\n",
      "5880/5880 [==============================] - 0s 39us/sample - loss: 0.4406 - acc: 0.8260 - val_loss: 0.4649 - val_acc: 0.8082\n",
      "Epoch 15/100\n",
      "5880/5880 [==============================] - 0s 38us/sample - loss: 0.4368 - acc: 0.8276 - val_loss: 0.4606 - val_acc: 0.8109\n",
      "Epoch 16/100\n",
      "5880/5880 [==============================] - 0s 37us/sample - loss: 0.4325 - acc: 0.8301 - val_loss: 0.4571 - val_acc: 0.8082\n",
      "Epoch 17/100\n",
      "5880/5880 [==============================] - 0s 37us/sample - loss: 0.4293 - acc: 0.8291 - val_loss: 0.4593 - val_acc: 0.8088\n",
      "Epoch 18/100\n",
      "5880/5880 [==============================] - 0s 37us/sample - loss: 0.4256 - acc: 0.8323 - val_loss: 0.4520 - val_acc: 0.8088\n",
      "Epoch 19/100\n",
      "5880/5880 [==============================] - 0s 39us/sample - loss: 0.4232 - acc: 0.8347 - val_loss: 0.4532 - val_acc: 0.8068\n",
      "Epoch 20/100\n",
      "5880/5880 [==============================] - 0s 37us/sample - loss: 0.4196 - acc: 0.8332 - val_loss: 0.4547 - val_acc: 0.8082\n",
      "Epoch 21/100\n",
      "5880/5880 [==============================] - 0s 37us/sample - loss: 0.4173 - acc: 0.8345 - val_loss: 0.4516 - val_acc: 0.8088\n",
      "Epoch 22/100\n",
      "5880/5880 [==============================] - 0s 37us/sample - loss: 0.4141 - acc: 0.8352 - val_loss: 0.4504 - val_acc: 0.8122\n",
      "Epoch 23/100\n",
      "5880/5880 [==============================] - 0s 37us/sample - loss: 0.4113 - acc: 0.8381 - val_loss: 0.4535 - val_acc: 0.8088\n",
      "Epoch 24/100\n",
      "5880/5880 [==============================] - 0s 38us/sample - loss: 0.4093 - acc: 0.8393 - val_loss: 0.4514 - val_acc: 0.8122\n",
      "Epoch 25/100\n",
      "5880/5880 [==============================] - 0s 39us/sample - loss: 0.4068 - acc: 0.8420 - val_loss: 0.4488 - val_acc: 0.8116\n",
      "Epoch 26/100\n",
      "5880/5880 [==============================] - 0s 37us/sample - loss: 0.4043 - acc: 0.8413 - val_loss: 0.4460 - val_acc: 0.8116\n",
      "Epoch 27/100\n",
      "5880/5880 [==============================] - 0s 37us/sample - loss: 0.4019 - acc: 0.8444 - val_loss: 0.4463 - val_acc: 0.8075\n",
      "Epoch 28/100\n",
      "5880/5880 [==============================] - 0s 37us/sample - loss: 0.4001 - acc: 0.8432 - val_loss: 0.4470 - val_acc: 0.8116\n",
      "Epoch 29/100\n",
      "5880/5880 [==============================] - 0s 37us/sample - loss: 0.3984 - acc: 0.8435 - val_loss: 0.4494 - val_acc: 0.8122\n",
      "Epoch 30/100\n",
      "5880/5880 [==============================] - 0s 37us/sample - loss: 0.3963 - acc: 0.8469 - val_loss: 0.4486 - val_acc: 0.8136\n",
      "Epoch 31/100\n",
      "5880/5880 [==============================] - 0s 37us/sample - loss: 0.3950 - acc: 0.8490 - val_loss: 0.4457 - val_acc: 0.8143\n",
      "Epoch 32/100\n",
      "5880/5880 [==============================] - 0s 37us/sample - loss: 0.3930 - acc: 0.8461 - val_loss: 0.4444 - val_acc: 0.8197\n",
      "Epoch 33/100\n",
      "5880/5880 [==============================] - 0s 38us/sample - loss: 0.3911 - acc: 0.8498 - val_loss: 0.4437 - val_acc: 0.8170\n",
      "Epoch 34/100\n",
      "5880/5880 [==============================] - 0s 39us/sample - loss: 0.3899 - acc: 0.8493 - val_loss: 0.4459 - val_acc: 0.8170\n",
      "Epoch 35/100\n",
      "5880/5880 [==============================] - 0s 39us/sample - loss: 0.3883 - acc: 0.8481 - val_loss: 0.4438 - val_acc: 0.8177\n",
      "Epoch 36/100\n",
      "5880/5880 [==============================] - 0s 41us/sample - loss: 0.3862 - acc: 0.8514 - val_loss: 0.4468 - val_acc: 0.8150\n",
      "Epoch 37/100\n",
      "5880/5880 [==============================] - 0s 37us/sample - loss: 0.3846 - acc: 0.8515 - val_loss: 0.4455 - val_acc: 0.8197\n",
      "Epoch 38/100\n",
      "5880/5880 [==============================] - 0s 37us/sample - loss: 0.3826 - acc: 0.8517 - val_loss: 0.4438 - val_acc: 0.8211\n",
      "Epoch 39/100\n",
      "5880/5880 [==============================] - 0s 39us/sample - loss: 0.3819 - acc: 0.8524 - val_loss: 0.4455 - val_acc: 0.8184\n",
      "Epoch 40/100\n",
      "5880/5880 [==============================] - 0s 41us/sample - loss: 0.3802 - acc: 0.8527 - val_loss: 0.4418 - val_acc: 0.8170\n",
      "Epoch 41/100\n",
      "5880/5880 [==============================] - 0s 37us/sample - loss: 0.3794 - acc: 0.8529 - val_loss: 0.4467 - val_acc: 0.8218\n",
      "Epoch 42/100\n",
      "5880/5880 [==============================] - 0s 37us/sample - loss: 0.3780 - acc: 0.8526 - val_loss: 0.4439 - val_acc: 0.8190\n",
      "Epoch 43/100\n",
      "5880/5880 [==============================] - 0s 37us/sample - loss: 0.3766 - acc: 0.8543 - val_loss: 0.4460 - val_acc: 0.8204\n",
      "Epoch 44/100\n",
      "5880/5880 [==============================] - 0s 41us/sample - loss: 0.3756 - acc: 0.8526 - val_loss: 0.4420 - val_acc: 0.8184\n",
      "Epoch 45/100\n",
      "5880/5880 [==============================] - 0s 39us/sample - loss: 0.3739 - acc: 0.8536 - val_loss: 0.4452 - val_acc: 0.8218\n",
      "Epoch 46/100\n",
      "5880/5880 [==============================] - 0s 34us/sample - loss: 0.3727 - acc: 0.8551 - val_loss: 0.4460 - val_acc: 0.8170\n",
      "Epoch 47/100\n",
      "5880/5880 [==============================] - 0s 39us/sample - loss: 0.3715 - acc: 0.8537 - val_loss: 0.4426 - val_acc: 0.8224\n",
      "Epoch 48/100\n",
      "5880/5880 [==============================] - 0s 35us/sample - loss: 0.3698 - acc: 0.8563 - val_loss: 0.4474 - val_acc: 0.8211\n",
      "Epoch 49/100\n",
      "5880/5880 [==============================] - 0s 39us/sample - loss: 0.3698 - acc: 0.8534 - val_loss: 0.4431 - val_acc: 0.8224\n",
      "Epoch 50/100\n",
      "5880/5880 [==============================] - 0s 34us/sample - loss: 0.3686 - acc: 0.8532 - val_loss: 0.4446 - val_acc: 0.8190\n",
      "Epoch 51/100\n",
      "5880/5880 [==============================] - 0s 39us/sample - loss: 0.3670 - acc: 0.8549 - val_loss: 0.4467 - val_acc: 0.8218\n",
      "Epoch 52/100\n",
      "5880/5880 [==============================] - 0s 39us/sample - loss: 0.3667 - acc: 0.8556 - val_loss: 0.4398 - val_acc: 0.8218\n",
      "Epoch 53/100\n",
      "5880/5880 [==============================] - 0s 63us/sample - loss: 0.3657 - acc: 0.8560 - val_loss: 0.4392 - val_acc: 0.8204\n",
      "Epoch 54/100\n",
      "5880/5880 [==============================] - 0s 58us/sample - loss: 0.3637 - acc: 0.8573 - val_loss: 0.4383 - val_acc: 0.8197\n",
      "Epoch 55/100\n",
      "5880/5880 [==============================] - 0s 51us/sample - loss: 0.3631 - acc: 0.8568 - val_loss: 0.4399 - val_acc: 0.8190\n",
      "Epoch 56/100\n",
      "5880/5880 [==============================] - 0s 47us/sample - loss: 0.3623 - acc: 0.8580 - val_loss: 0.4418 - val_acc: 0.8197\n",
      "Epoch 57/100\n",
      "5880/5880 [==============================] - 0s 38us/sample - loss: 0.3614 - acc: 0.8561 - val_loss: 0.4396 - val_acc: 0.8218\n",
      "Epoch 58/100\n",
      "5880/5880 [==============================] - 0s 37us/sample - loss: 0.3603 - acc: 0.8612 - val_loss: 0.4427 - val_acc: 0.8211\n",
      "Epoch 59/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5880/5880 [==============================] - 0s 37us/sample - loss: 0.3593 - acc: 0.8585 - val_loss: 0.4392 - val_acc: 0.8197\n",
      "Epoch 60/100\n",
      "5880/5880 [==============================] - 0s 34us/sample - loss: 0.3585 - acc: 0.8592 - val_loss: 0.4439 - val_acc: 0.8190\n",
      "Epoch 61/100\n",
      "5880/5880 [==============================] - 0s 37us/sample - loss: 0.3580 - acc: 0.8600 - val_loss: 0.4433 - val_acc: 0.8218\n",
      "Epoch 62/100\n",
      "5880/5880 [==============================] - 0s 38us/sample - loss: 0.3563 - acc: 0.8622 - val_loss: 0.4405 - val_acc: 0.8197\n",
      "Epoch 63/100\n",
      "5880/5880 [==============================] - 0s 47us/sample - loss: 0.3548 - acc: 0.8619 - val_loss: 0.4401 - val_acc: 0.8177\n",
      "Epoch 64/100\n",
      "5880/5880 [==============================] - 0s 34us/sample - loss: 0.3549 - acc: 0.8609 - val_loss: 0.4403 - val_acc: 0.8197\n",
      "Epoch 65/100\n",
      "5880/5880 [==============================] - 0s 34us/sample - loss: 0.3537 - acc: 0.8607 - val_loss: 0.4390 - val_acc: 0.8238\n",
      "Epoch 66/100\n",
      "5880/5880 [==============================] - 0s 48us/sample - loss: 0.3527 - acc: 0.8646 - val_loss: 0.4413 - val_acc: 0.8224\n",
      "Epoch 67/100\n",
      "5880/5880 [==============================] - 0s 35us/sample - loss: 0.3517 - acc: 0.8631 - val_loss: 0.4395 - val_acc: 0.8238\n",
      "Epoch 68/100\n",
      "5880/5880 [==============================] - 0s 36us/sample - loss: 0.3507 - acc: 0.8628 - val_loss: 0.4405 - val_acc: 0.8190\n",
      "Epoch 69/100\n",
      "5880/5880 [==============================] - 0s 34us/sample - loss: 0.3504 - acc: 0.8612 - val_loss: 0.4414 - val_acc: 0.8163\n",
      "Epoch 70/100\n",
      "5880/5880 [==============================] - 0s 42us/sample - loss: 0.3485 - acc: 0.8634 - val_loss: 0.4422 - val_acc: 0.8156\n",
      "Epoch 71/100\n",
      "5880/5880 [==============================] - 0s 37us/sample - loss: 0.3489 - acc: 0.8629 - val_loss: 0.4401 - val_acc: 0.8204\n",
      "Epoch 72/100\n",
      "5880/5880 [==============================] - 0s 37us/sample - loss: 0.3473 - acc: 0.8639 - val_loss: 0.4416 - val_acc: 0.8204\n",
      "Epoch 73/100\n",
      "5880/5880 [==============================] - 0s 34us/sample - loss: 0.3461 - acc: 0.8638 - val_loss: 0.4395 - val_acc: 0.8224\n",
      "Epoch 74/100\n",
      "5880/5880 [==============================] - 0s 43us/sample - loss: 0.3456 - acc: 0.8651 - val_loss: 0.4388 - val_acc: 0.8231\n",
      "Epoch 75/100\n",
      "5880/5880 [==============================] - 0s 37us/sample - loss: 0.3448 - acc: 0.8660 - val_loss: 0.4381 - val_acc: 0.8190\n",
      "Epoch 76/100\n",
      "5880/5880 [==============================] - 0s 37us/sample - loss: 0.3446 - acc: 0.8646 - val_loss: 0.4419 - val_acc: 0.8177\n",
      "Epoch 77/100\n",
      "5880/5880 [==============================] - 0s 43us/sample - loss: 0.3427 - acc: 0.8673 - val_loss: 0.4387 - val_acc: 0.8197\n",
      "Epoch 78/100\n",
      "5880/5880 [==============================] - 0s 39us/sample - loss: 0.3423 - acc: 0.8651 - val_loss: 0.4398 - val_acc: 0.8211\n",
      "Epoch 79/100\n",
      "5880/5880 [==============================] - 0s 38us/sample - loss: 0.3419 - acc: 0.8663 - val_loss: 0.4427 - val_acc: 0.8190\n",
      "Epoch 80/100\n",
      "5880/5880 [==============================] - 0s 47us/sample - loss: 0.3405 - acc: 0.8646 - val_loss: 0.4413 - val_acc: 0.8197\n",
      "Epoch 81/100\n",
      "5880/5880 [==============================] - 0s 41us/sample - loss: 0.3406 - acc: 0.8645 - val_loss: 0.4429 - val_acc: 0.8218\n",
      "Epoch 82/100\n",
      "5880/5880 [==============================] - 0s 45us/sample - loss: 0.3385 - acc: 0.8668 - val_loss: 0.4419 - val_acc: 0.8211\n",
      "Epoch 83/100\n",
      "5880/5880 [==============================] - 0s 51us/sample - loss: 0.3390 - acc: 0.8677 - val_loss: 0.4391 - val_acc: 0.8245\n",
      "Epoch 84/100\n",
      "5880/5880 [==============================] - 0s 41us/sample - loss: 0.3377 - acc: 0.8672 - val_loss: 0.4431 - val_acc: 0.8190\n",
      "Epoch 85/100\n",
      "5880/5880 [==============================] - 0s 39us/sample - loss: 0.3370 - acc: 0.8662 - val_loss: 0.4396 - val_acc: 0.8224\n",
      "Epoch 86/100\n",
      "5880/5880 [==============================] - 0s 43us/sample - loss: 0.3363 - acc: 0.8663 - val_loss: 0.4387 - val_acc: 0.8231\n",
      "Epoch 87/100\n",
      "5880/5880 [==============================] - 0s 42us/sample - loss: 0.3356 - acc: 0.8690 - val_loss: 0.4402 - val_acc: 0.8224\n",
      "Epoch 88/100\n",
      "5880/5880 [==============================] - 0s 41us/sample - loss: 0.3364 - acc: 0.8711 - val_loss: 0.4471 - val_acc: 0.8184\n",
      "Epoch 89/100\n",
      "5880/5880 [==============================] - 0s 45us/sample - loss: 0.3341 - acc: 0.8675 - val_loss: 0.4427 - val_acc: 0.8184\n",
      "Epoch 90/100\n",
      "5880/5880 [==============================] - 0s 38us/sample - loss: 0.3341 - acc: 0.8675 - val_loss: 0.4431 - val_acc: 0.8184\n",
      "Epoch 91/100\n",
      "5880/5880 [==============================] - 0s 51us/sample - loss: 0.3330 - acc: 0.8721 - val_loss: 0.4428 - val_acc: 0.8204\n",
      "Epoch 92/100\n",
      "5880/5880 [==============================] - 0s 42us/sample - loss: 0.3321 - acc: 0.8713 - val_loss: 0.4460 - val_acc: 0.8156\n",
      "Epoch 93/100\n",
      "5880/5880 [==============================] - 0s 39us/sample - loss: 0.3323 - acc: 0.8667 - val_loss: 0.4447 - val_acc: 0.8211\n",
      "Epoch 94/100\n",
      "5880/5880 [==============================] - 0s 38us/sample - loss: 0.3308 - acc: 0.8680 - val_loss: 0.4426 - val_acc: 0.8204\n",
      "Epoch 95/100\n",
      "5880/5880 [==============================] - 0s 47us/sample - loss: 0.3311 - acc: 0.8689 - val_loss: 0.4430 - val_acc: 0.8197\n",
      "Epoch 96/100\n",
      "5880/5880 [==============================] - 0s 38us/sample - loss: 0.3296 - acc: 0.8685 - val_loss: 0.4422 - val_acc: 0.8204\n",
      "Epoch 97/100\n",
      "5880/5880 [==============================] - 0s 45us/sample - loss: 0.3298 - acc: 0.8689 - val_loss: 0.4457 - val_acc: 0.8204\n",
      "Epoch 98/100\n",
      "5880/5880 [==============================] - 0s 41us/sample - loss: 0.3282 - acc: 0.8679 - val_loss: 0.4453 - val_acc: 0.8224\n",
      "Epoch 99/100\n",
      "5880/5880 [==============================] - 0s 45us/sample - loss: 0.3280 - acc: 0.8701 - val_loss: 0.4457 - val_acc: 0.8170\n",
      "Epoch 100/100\n",
      "5880/5880 [==============================] - 0s 41us/sample - loss: 0.3278 - acc: 0.8675 - val_loss: 0.4429 - val_acc: 0.8218\n"
     ]
    }
   ],
   "source": [
    "model_history = model.fit(X_train, y_train, epochs=100, batch_size=32, shuffle=True, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7350/7350 [==============================] - 0s 34us/sample - loss: 0.3444 - acc: 0.8645\n",
      "3150/3150 [==============================] - 0s 27us/sample - loss: 0.4417 - acc: 0.8219\n",
      "Accuracy Training: 0.8644898\n",
      "Accuracy Testing: 0.8219048\n"
     ]
    }
   ],
   "source": [
    "res_loss_train, accuracy_train = model.evaluate(X_train, y_train)\n",
    "res_loss_test, accuracy_test = model.evaluate(X_test, y_test)\n",
    "print(\"Accuracy Training:\",accuracy_train)\n",
    "print(\"Accuracy Testing:\",accuracy_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Regularisation (1.5 marks)\n",
    "---\n",
    "This section covers **2** exercises on the impact of weight regularisaton. Note that $L_{1}$- and $L_{2}$-regularisation may also be applied to the activation of intermediate layers. Also, a similar regularising effect could be achieved using **dropout** regularisation, which you are encouraged to try out, but which we won't study in this CP exercise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Relevant import(s) for this section\n",
    "from tensorflow.python.keras.regularizers import l1_l2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 14. Define `Keras` model factory method (0.5 mark)\n",
    "\n",
    "- Define a python function called `big_model_fn` which takes the followng three arguments:\n",
    "    - `l1`: A float specifying the $L_{1}$ regularisation factor (default value: 0)\n",
    "    - `l2`: A float specifying the $L_{2}$ regularisation factor (default value: 0)\n",
    "    - `name`: A string, specifying the name of the model (default value: None)\n",
    "- Indside the function, you should:\n",
    "    - Construct a `Keras` model using the functional API, which has:\n",
    "        - An input layer with the same number of nodes as the number of features in `X`.\n",
    "        - **Two** densely connected hidden layer with **100 nodes** each, both equipped with **ReLU activation**.\n",
    "        - Both hidden layers should be subject to kernel regularisation (_i.e._ weight regularisation) with the regularisation factors specified as an input.\n",
    "        - A densely connected output layer with **3 nodes** (the number of types of weather we're classifying) equipped with **softmax activation**.\n",
    "        - A name given by the corresponding argument.\n",
    "    - Compile the model in the same way as in **Exercise 14.**\n",
    "- The function should return the compiled `Keras` model. \n",
    "\n",
    "The method will provide a convenient way of constructing and compiling a number of \"big\"/deep `Keras` models which differ only by their regularisation and name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 15. Train \"big\" models with and without regularisation (1 mark)\n",
    "\n",
    "- Construct three \"big\" model using the factory method:\n",
    "     - One with default parameters\n",
    "     - One with `l1=0.0003` and  `name='Big model (L1-regularised)'`\n",
    "     - One with `l2=0.003`  and `name='Big model (L2-regularised)'`\n",
    "- Train each one as in **Exercise 15.**\n",
    "- Compare first the loss history of the un-regularised \"big\" model to that of the small model from **Exercise 15** using the `plot.loss()` method.\n",
    "- Then, compare the loss histories of all three \"big\" models with that of the small model.\n",
    "- Plot the loss and val loss of all 4 models and discuss the results. Target these points:\n",
    "    - Compare the performance of deep vs shallow models on the testing sets\n",
    "    - Compare the level of ovetraining (training vs testing loss)\n",
    "    - Note: Don't be alarmed if the shallow network performs slightly better that the deeper ones, this is dataset dependant."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Bonus: Hyperparameter optimisation (1\\*bonus\\* mark)\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section covers **1** exercise on the on hyperparameter optimisation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Relevant import(s) for this section\n",
    "from sklearn.model_selection import GridSearchCV, cross_val_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "_**Comment on simplified hyperparameter optimisation example:**_ You will try to perform a simple optimisation using a grid search\n",
    "\n",
    "For convenience, we will be using the `scikit-learn` `MLPClassifier` as our base class, but the same principles apply to just about any ML model constructed in any framework. Just as in the examples in the lecture, we will restrict the hyperparameter space to just two dimensions:\n",
    "\n",
    "* the number of hidden layers, `nb_layers`, and\n",
    "* the number of nodes per hidden layer, `nb_nodes_per_layer`, which is taken to be the same for all hidden layers for simplicity.\n",
    "\n",
    "Since the `scikit-learn` neural network classifier class doesn't support these two hyperparameters by default, provided is a simple wrapper class, that works exactly like `MLPClassifier`, it just takes the two parameters above as arguments in the constructor. Don't worry about understanding it in detail. This allows us to call "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLPClassifierWrapper(MLPClassifier):\n",
    "    \"\"\"\n",
    "    Wrapper around `sklearn.neural_network.MLPClassifier` with a convenient set \n",
    "    of properties (nb_layers and nb_nodes_per_layer) suitable for hyperparameter \n",
    "    optimisation exercises.\n",
    "    \n",
    "    Arguments:\n",
    "        nb_layers: Integer, number of hidden layers\n",
    "        nb_nodes_per_layer: Number of nodes per hidden layer, taken to be the \n",
    "            same for all for convenience.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__ (self, nb_layers=1, nb_nodes_per_layer=100, **kwargs):\n",
    "        \n",
    "        # Member variables\n",
    "        self._nb_layers = nb_layers\n",
    "        self._nb_nodes_per_layer = nb_nodes_per_layer  \n",
    "        \n",
    "        # Call base class (`MLPClassifier`) constructor\n",
    "        super(MLPClassifierWrapper, self).__init__(**kwargs)\n",
    "        \n",
    "        # Trigger `_set_architecture`\n",
    "        self._set_architecture()\n",
    "        return\n",
    "\n",
    "    @property\n",
    "    def nb_layers(self):\n",
    "        return self._nb_layers\n",
    "    \n",
    "    @property\n",
    "    def nb_nodes_per_layer(self):\n",
    "        return self._nb_nodes_per_layer \n",
    "\n",
    "    @nb_layers.setter\n",
    "    def nb_layers(self, value):\n",
    "        self._nb_layers = value\n",
    "        self._set_architecture()\n",
    "        return\n",
    "    \n",
    "    @nb_nodes_per_layer.setter\n",
    "    def nb_nodes_per_layer(self, value):\n",
    "        self._nb_nodes_per_layer = value\n",
    "        self._set_architecture()\n",
    "        return\n",
    "    \n",
    "    def _set_architecture (self):\n",
    "        \"\"\"\n",
    "        Sets the `hidden_layer_sizes` parameter of the base `MLPClassifier` \n",
    "        class, based on the two custom parameters we have chosen.\n",
    "        \"\"\"\n",
    "        \n",
    "        self.hidden_layer_sizes = tuple([self._nb_nodes_per_layer for _ in range(self._nb_layers)])\n",
    "        return\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 16. Perform a grid search (1 mark)\n",
    "\n",
    "- Construct a python `dict` called `param_grid` which specifies the hyperparameter configurations to try for each parameter dimension. That is, it should have\n",
    "    - `\"nb_layers\"` and `\"nb_nodes_per_layer\"` as keys, and\n",
    "    - lists of integers as values, corresponding to the values of each parameter you want to try out (_e.g._ [1, 2, ...])\n",
    "- Choose a reasonable set of values for each parameter; about a handful for each.\n",
    "- Use the `GridSearchCV` class to perform _**3**_**-fold** cross validation (CV) optimisation of the validation **accuracy**\n",
    "    - Hint: You can use the `n_jobs=...` argument to enable multi-processing, thereby speeding up the optimisation, at the expense of reproducibility.\n",
    "- The base classifier should be an instance of `MLPClassifierWrapper` set to train for **100 epochs**.\n",
    "- Present the results:\n",
    "    - Print the best parameter configuration found. GridSearchCV has a public member which stores this. Read doc.\n",
    "    - Print the mean and standard deviation of the test scores for the best configuration found. (_Hint:_ These can be found in the `.cv_results_` attribute)\n",
    "    - Plot the optimisation results using the `plot.optimisation` method.\n",
    "- Discuss the results. What would happen if the best result is foundon the edge of the parameter grid?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
